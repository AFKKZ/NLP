{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Biểu diễn Doc2vec sử dụng Deep Learning\n",
    "\n",
    "## Có sử dụng pretrained models\n",
    "\n",
    "Notebook này triển khai mô hình Doc2vec sử dụng Deep Learning với pretrained model Word2Vec. Cụ thể:\n",
    "\n",
    "1. Sử dụng pretrained Word2Vec (GloVe) làm khởi tạo cho word embeddings\n",
    "2. Xây dựng mô hình Deep Neural Network với nhiều lớp ẩn để tạo document embeddings\n",
    "3. Kết hợp cả hai để biểu diễn văn bản dưới dạng vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Thiết lập logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltk-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## 1. Tạo dữ liệu mẫu\n",
    "\n",
    "Tạo tập dữ liệu văn bản mẫu để thực nghiệm với Doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo dữ liệu mẫu\n",
    "def create_sample_data(filename='sample_data.csv'):\n",
    "    \"\"\"\n",
    "    Tạo tập dữ liệu mẫu và lưu thành CSV\n",
    "    \"\"\"\n",
    "    sample_texts = [\n",
    "        \"Deep learning enables machine learning models to learn hierarchical representations.\",\n",
    "        \"Neural networks are the foundation of most deep learning methods.\",\n",
    "        \"Word embeddings capture semantic relationships between words in a text corpus.\",\n",
    "        \"Document embeddings represent entire documents in a vector space.\",\n",
    "        \"Natural language processing uses computational techniques to analyze text.\",\n",
    "        \"Transfer learning leverages knowledge from pretrained models for new tasks.\",\n",
    "        \"Convolutional neural networks excel at computer vision tasks.\",\n",
    "        \"Recurrent neural networks are effective for sequential data like text.\",\n",
    "        \"Attention mechanisms help models focus on relevant parts of the input data.\",\n",
    "        \"Transformers have revolutionized natural language understanding tasks.\",\n",
    "        \"Doc2Vec extends Word2Vec to create document-level embeddings.\",\n",
    "        \"Unsupervised learning discovers patterns without labeled training data.\",\n",
    "        \"Semantic analysis aims to understand the meaning of text data.\",\n",
    "        \"Python is the most popular programming language for deep learning.\",\n",
    "        \"PyTorch provides a flexible framework for building neural networks.\"\n",
    "    ]\n",
    "    \n",
    "    # Tạo labels cho task phân loại (để demo)\n",
    "    labels = ['technique', 'architecture', 'embedding', 'embedding', 'technique',\n",
    "              'technique', 'architecture', 'architecture', 'component', 'architecture',\n",
    "              'embedding', 'technique', 'technique', 'tool', 'tool']\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': sample_texts,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Lưu thành file CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Đã tạo file {filename} với {len(df)} văn bản\")\n",
    "    return df\n",
    "\n",
    "# Tạo dữ liệu mẫu nếu chưa có\n",
    "if not os.path.exists('sample_data.csv'):\n",
    "    df = create_sample_data()\n",
    "else:\n",
    "    df = pd.read_csv('sample_data.csv')\n",
    "    print(f\"Đã đọc file sample_data.csv với {len(df)} văn bản\")\n",
    "    \n",
    "# Hiển thị 5 mẫu đầu tiên\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretrained-model-section",
   "metadata": {},
   "source": [
    "## 2. Tải Pretrained Word Embeddings\n",
    "\n",
    "Tải pretrained word embeddings từ Gensim. Chúng ta sử dụng GloVe với 100 chiều để giảm thời gian tải và tiết kiệm bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-pretrained",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_word_embeddings(model_name='glove-wiki-gigaword-100'):\n",
    "    \"\"\"\n",
    "    Tải pretrained word embeddings và lưu ở định dạng memory-mapped\n",
    "    \"\"\"\n",
    "    pretrained_path = f\"{model_name.replace('-', '_')}.bin\"\n",
    "    mmap_path = f\"{model_name.replace('-', '_')}.mmap\"\n",
    "    \n",
    "    if not os.path.exists(mmap_path):\n",
    "        if not os.path.exists(pretrained_path):\n",
    "            print(f\"Đang tải model {model_name}...\")\n",
    "            start_time = time.time()\n",
    "            word_vectors = load(model_name)\n",
    "            print(f\"Đã tải xong sau {time.time() - start_time:.2f} giây\")\n",
    "            \n",
    "            print(\"Đang lưu model dưới dạng binary...\")\n",
    "            word_vectors.save_word2vec_format(pretrained_path, binary=True)\n",
    "            print(f\"Đã lưu model tại: {pretrained_path}\")\n",
    "        \n",
    "        # Chuyển đổi sang định dạng memory-mapped để tối ưu RAM\n",
    "        print(\"Đang chuyển sang định dạng memory-mapped...\")\n",
    "        temp_wv = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "        temp_wv.save(mmap_path)\n",
    "        vector_size = temp_wv.vector_size\n",
    "        vocab_size = len(temp_wv)\n",
    "        del temp_wv  # Giải phóng bộ nhớ\n",
    "        print(f\"Đã lưu memory-mapped model tại: {mmap_path}\")\n",
    "        print(f\"Kích thước từ điển: {vocab_size}, Kích thước vector: {vector_size}\")\n",
    "    else:\n",
    "        print(f\"Đang tải memory-mapped model từ: {mmap_path}\")\n",
    "        temp_wv = KeyedVectors.load(mmap_path, mmap='r')\n",
    "        vector_size = temp_wv.vector_size\n",
    "        vocab_size = len(temp_wv)\n",
    "        del temp_wv  # Giải phóng bộ nhớ\n",
    "        print(f\"Kích thước từ điển: {vocab_size}, Kích thước vector: {vector_size}\")\n",
    "    \n",
    "    return mmap_path, vector_size\n",
    "\n",
    "# Tải pretrained word embeddings\n",
    "pretrained_path, VECTOR_SIZE = download_word_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-dataset-section",
   "metadata": {},
   "source": [
    "## 3. Xây dựng Dataset và Preprocessing\n",
    "\n",
    "Xây dựng các class để xử lý dữ liệu văn bản và chuẩn bị cho việc huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, texts, max_length=50):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset từ danh sách các văn bản\n",
    "        texts: list các văn bản\n",
    "        max_length: độ dài tối đa của mỗi văn bản\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}  # Thêm token đặc biệt\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "        \n",
    "        # Xây dựng từ điển\n",
    "        self._build_vocab()\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        \"\"\"\n",
    "        Xây dựng từ điển từ tất cả các văn bản\n",
    "        \"\"\"\n",
    "        word_freq = defaultdict(int)\n",
    "        \n",
    "        # Đếm tần suất từ\n",
    "        for text in self.texts:\n",
    "            words = word_tokenize(text.lower())\n",
    "            for word in words:\n",
    "                word_freq[word] += 1\n",
    "        \n",
    "        # Tạo từ điển với các từ xuất hiện ít nhất 1 lần\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= 1 and word not in self.word2idx:\n",
    "                self.word2idx[word] = self.vocab_size\n",
    "                self.idx2word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "        print(f\"Kích thước từ điển: {self.vocab_size}\")\n",
    "        \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"\n",
    "        Chuyển văn bản thành chuỗi các index\n",
    "        \"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in words][:self.max_length]\n",
    "\n",
    "# Chia dữ liệu thành train và validation\n",
    "train_texts, val_texts = train_test_split(df['text'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tạo dataset cho các văn bản\n",
    "text_dataset = TextDataset(train_texts + val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doc2vec-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecDataset(Dataset):\n",
    "    def __init__(self, texts, text_dataset, window_size=5):\n",
    "        \"\"\"\n",
    "        Dataset cho Doc2Vec\n",
    "        texts: danh sách các văn bản\n",
    "        text_dataset: TextDataset object chứa từ điển\n",
    "        window_size: kích thước cửa sổ ngữ cảnh\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.text_dataset = text_dataset\n",
    "        self.window_size = window_size\n",
    "        self.data = self._prepare_training_data()\n",
    "        \n",
    "    def _prepare_training_data(self):\n",
    "        \"\"\"\n",
    "        Chuẩn bị dữ liệu huấn luyện với cửa sổ ngữ cảnh\n",
    "        Format: (doc_id, target_word_id, context_word_id)\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for doc_id, text in enumerate(self.texts):\n",
    "            word_indices = self.text_dataset.tokenize_text(text)\n",
    "            \n",
    "            for target_pos, target_idx in enumerate(word_indices):\n",
    "                # Lấy các từ trong cửa sổ ngữ cảnh\n",
    "                context_indices = []\n",
    "                start = max(0, target_pos - self.window_size)\n",
    "                end = min(len(word_indices), target_pos + self.window_size + 1)\n",
    "                \n",
    "                for i in range(start, end):\n",
    "                    if i != target_pos:  # Bỏ qua target word\n",
    "                        context_indices.append(word_indices[i])\n",
    "                \n",
    "                for context_idx in context_indices:\n",
    "                    training_data.append((doc_id, target_idx, context_idx))\n",
    "                    \n",
    "        return training_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, target_idx, context_idx = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(doc_id, dtype=torch.long),\n",
    "            torch.tensor(target_idx, dtype=torch.long),\n",
    "            torch.tensor(context_idx, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# Tạo dataset cho train và validation\n",
    "train_dataset = Doc2VecDataset(train_texts, text_dataset)\n",
    "val_dataset = Doc2VecDataset(val_texts, text_dataset)\n",
    "\n",
    "print(f\"Số lượng mẫu huấn luyện: {len(train_dataset)}\")\n",
    "print(f\"Số lượng mẫu validation: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## 4. Xây dựng Mô Hình Doc2Vec với Deep Learning\n",
    "\n",
    "Xây dựng mô hình Doc2Vec sử dụng Deep Learning với pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doc2vec-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecWithPretrained(nn.Module):\n",
    "    def __init__(self, pretrained_path, vocab_size, doc_count, vector_size=100):\n",
    "        \"\"\"\n",
    "        Mô hình Doc2Vec với pretrained embeddings và deep learning layers\n",
    "        pretrained_path: đường dẫn đến pretrained model\n",
    "        vocab_size: kích thước từ điển\n",
    "        doc_count: số lượng văn bản\n",
    "        vector_size: kích thước vector\n",
    "        \"\"\"\n",
    "        super(Doc2VecWithPretrained, self).__init__()\n",
    "        \n",
    "        self.vector_size = vector_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, vector_size)\n",
    "        \n",
    "        # Document embeddings\n",
    "        self.doc_embeddings = nn.Embedding(doc_count, vector_size)\n",
    "        \n",
    "        # Khởi tạo document embeddings\n",
    "        nn.init.uniform_(self.doc_embeddings.weight, -0.1, 0.1)\n",
    "        \n",
    "        # Deep learning layers - Kiến trúc sâu với nhiều lớp ẩn\n",
    "        self.fc1 = nn.Linear(vector_size * 2, vector_size * 2)\n",
    "        self.fc2 = nn.Linear(vector_size * 2, vector_size)\n",
    "        self.fc3 = nn.Linear(vector_size, vector_size)\n",
    "        self.output = nn.Linear(vector_size, vocab_size)\n",
    "        \n",
    "        # Các layer khác\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(vector_size * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(vector_size)\n",
    "        \n",
    "        # Khởi tạo word embeddings từ pretrained model\n",
    "        self._init_word_embeddings(pretrained_path)\n",
    "        \n",
    "    def _init_word_embeddings(self, pretrained_path):\n",
    "        \"\"\"\n",
    "        Khởi tạo word embeddings từ pretrained model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Khởi tạo ngẫu nhiên trước\n",
    "            nn.init.uniform_(self.word_embeddings.weight, -0.1, 0.1)\n",
    "            \n",
    "            # Tải pretrained model\n",
    "            pretrained_vecs = KeyedVectors.load(pretrained_path, mmap='r')\n",
    "            \n",
    "            # Map từ word2idx tới pretrained vectors\n",
    "            initialized = 0\n",
    "            for word, idx in self.word2idx.items():\n",
    "                if word in pretrained_vecs:\n",
    "                    self.word_embeddings.weight.data[idx] = torch.FloatTensor(pretrained_vecs[word])\n",
    "                    initialized += 1\n",
    "                elif word.lower() in pretrained_vecs:  # Thử lowercase\n",
    "                    self.word_embeddings.weight.data[idx] = torch.FloatTensor(pretrained_vecs[word.lower()])\n",
    "                    initialized += 1\n",
    "            \n",
    "            print(f\"Đã khởi tạo {initialized}/{self.vocab_size} từ từ pretrained vectors\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi khởi tạo word embeddings: {e}\")\n",
    "\n",
    "    def forward(self, doc_ids, word_ids):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        doc_ids: tensor chứa document IDs\n",
    "        word_ids: tensor chứa word IDs\n",
    "        \"\"\"\n",
    "        # Lấy embeddings\n",
    "        doc_embeds = self.doc_embeddings(doc_ids)\n",
    "        word_embeds = self.word_embeddings(word_ids)\n",
    "        \n",
    "        # Kết hợp document và word embeddings\n",
    "        combined = torch.cat([doc_embeds, word_embeds], dim=1)\n",
    "        \n",
    "        # Deep learning layers\n",
    "        x = self.activation(self.fc1(combined))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## 5. Huấn luyện mô hình\n",
    "\n",
    "Thiết lập quá trình huấn luyện và validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(model, train_loader, val_loader, epochs=5, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Huấn luyện mô hình Doc2Vec\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Sử dụng device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer với learning rates khác nhau cho word embeddings và doc embeddings\n",
    "    word_embed_params = {'params': model.word_embeddings.parameters(), 'lr': learning_rate * 0.1}\n",
    "    doc_embed_params = {'params': model.doc_embeddings.parameters(), 'lr': learning_rate}\n",
    "    other_params = {'params': [p for n, p in model.named_parameters() \n",
    "                             if 'word_embeddings' not in n and 'doc_embeddings' not in n],\n",
    "                  'lr': learning_rate, 'weight_decay': 1e-5}\n",
    "    \n",
    "    optimizer = optim.Adam([word_embed_params, doc_embed_params, other_params])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1, verbose=True)\n",
    "    \n",
    "    # Để theo dõi tiến trình\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (doc_ids, word_ids, context_ids) in enumerate(train_loader):\n",
    "            # Chuyển dữ liệu lên device\n",
    "            doc_ids = doc_ids.to(device)\n",
    "            word_ids = word_ids.to(device)\n",
    "            context_ids = context_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(doc_ids, word_ids)\n",
    "            loss = criterion(outputs, context_ids)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Giải phóng bộ nhớ\n",
    "            del doc_ids, word_ids, context_ids, outputs\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # In tiến trình\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs} [{batch_idx}/{len(train_loader)}] - Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Tính loss trung bình trên tập train\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for doc_ids, word_ids, context_ids in val_loader:\n",
    "                # Chuyển dữ liệu lên device\n",
    "                doc_ids = doc_ids.to(device)\n",
    "                word_ids = word_ids.to(device)\n",
    "                context_ids = context_ids.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(doc_ids, word_ids)\n",
    "                loss = criterion(outputs, context_ids)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Giải phóng bộ nhớ\n",
    "                del doc_ids, word_ids, context_ids, outputs\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        # Tính loss trung bình trên tập validation\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Điều chỉnh learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Lưu model tốt nhất\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_doc2vec_model.pth')\n",
    "            print(f'Model được lưu với val_loss: {best_val_loss:.4f}')\n",
    "            counter = 0  # Reset counter\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping sau {epoch+1} epochs')\n",
    "            break\n",
    "            \n",
    "        # In kết quả epoch\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{epochs} kết thúc sau {time_elapsed:.1f}s - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Vẽ đồ thị loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('doc2vec_training_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn bị DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Khởi tạo model\n",
    "doc_count = len(train_texts) + len(val_texts)\n",
    "model = Doc2VecWithPretrained(\n",
    "    pretrained_path=pretrained_path,\n",
    "    vocab_size=text_dataset.vocab_size,\n",
    "    doc_count=doc_count,\n",
    "    vector_size=VECTOR_SIZE\n",
    ")\n",
    "\n",
    "# Gán từ điển cho model để khởi tạo embeddings\n",
    "model.word2idx = text_dataset.word2idx\n",
    "\n",
    "# Huấn luyện model\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "train_losses, val_losses = train_doc2vec(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-embeddings-section",
   "metadata": {},
   "source": [
    "## 6. Tạo Document Embeddings\n",
    "\n",
    "Sử dụng mô hình đã huấn luyện để tạo document embeddings cho các văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-document-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embeddings(model, texts, text_dataset):\n",
    "    \"\"\"\n",
    "    Tạo document embeddings cho danh sách các văn bản\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize text\n",
    "            word_indices = text_dataset.tokenize_text(text)\n",
    "            if len(word_indices) == 0:  # Skip empty texts\n",
    "                doc_embeddings.append(np.zeros(model.vector_size))\n",
    "                continue\n",
    "                \n",
    "            # Convert to tensors\n",
    "            word_indices = torch.tensor(word_indices, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get word embeddings\n",
    "            word_embeds = model.word_embeddings(word_indices)\n",
    "            \n",
    "            # Average word embeddings to get document embedding\n",
    "            doc_embedding = torch.mean(word_embeds, dim=0).cpu().numpy()\n",
    "            doc_embeddings.append(doc_embedding)\n",
    "    \n",
    "    return np.array(doc_embeddings)\n",
    "\n",
    "# Tải model tốt nhất\n",
    "best_model = Doc2VecWithPretrained(\n",
    "    pretrained_path=pretrained_path,\n",
    "    vocab_size=text_dataset.vocab_size,\n",
    "    doc_count=doc_count,\n",
    "    vector_size=VECTOR_SIZE\n",
    ")\n",
    "best_model.word2idx = text_dataset.word2idx\n",
    "\n",
    "try:\n",
    "    best_model.load_state_dict(torch.load('best_doc2vec_model.pth'))\n",
    "    print(\"Đã tải model tốt nhất từ file\")\n",
    "except:\n",
    "    print(\"Không tìm thấy file model tốt nhất, sử dụng model hiện tại\")\n",
    "    best_model = model\n",
    "\n",
    "# Tạo document embeddings\n",
    "all_texts = df['text'].tolist()\n",
    "all_labels = df['label'].tolist()\n",
    "doc_embeddings = get_document_embeddings(best_model, all_texts, text_dataset)\n",
    "\n",
    "print(f\"Kích thước document embeddings: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-section",
   "metadata": {},
   "source": [
    "## 7. Trực quan hóa Document Embeddings\n",
    "\n",
    "Trực quan hóa document embeddings sử dụng t-SNE để giảm chiều xuống 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Trực quan hóa embeddings bằng t-SNE\n",
    "    \"\"\"\n",
    "    # Sử dụng t-SNE để giảm chiều\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Tạo DataFrame để visualization\n",
    "    vis_df = pd.DataFrame({\n",
    "        'x': reduced_embeddings[:, 0],\n",
    "        'y': reduced_embeddings[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Các nhãn duy nhất\n",
    "    unique_labels = vis_df['label'].unique()\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        subset = vis_df[vis_df['label'] == label]\n",
    "        plt.scatter(subset['x'], subset['y'], c=[colors[i]], label=label, alpha=0.7)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE Visualization of Document Embeddings')\n",
    "    plt.savefig('doc2vec_visualization.png')\n",
    "    plt.show()\n",
    "\n",
    "# Trực quan hóa\n",
    "visualize_embeddings(doc_embeddings, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-usage-section",
   "metadata": {},
   "source": [
    "## 8. Ứng dụng: Tìm kiếm văn bản tương tự\n",
    "\n",
    "Sử dụng document embeddings để tìm các văn bản tương tự với một văn bản mới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(doc_embedding, all_embeddings):\n",
    "    \"\"\"\n",
    "    Tính độ tương tự cosine giữa một document embedding với tất cả các document embeddings khác\n",
    "    \"\"\"\n",
    "    # Normalize các vector\n",
    "    doc_norm = np.linalg.norm(doc_embedding)\n",
    "    all_norms = np.linalg.norm(all_embeddings, axis=1)\n",
    "    \n",
    "    # Tính cosine similarity\n",
    "    similarities = np.dot(all_embeddings, doc_embedding) / (all_norms * doc_norm)\n",
    "    return similarities\n",
    "\n",
    "def find_similar_documents(query_text, model, text_dataset, all_texts, top_n=3):\n",
    "    \"\"\"\n",
    "    Tìm top_n văn bản tương tự với query_text\n",
    "    \"\"\"\n",
    "    # Tính document embedding cho query\n",
    "    query_embedding = get_document_embeddings(model, [query_text], text_dataset)[0]\n",
    "    \n",
    "    # Tính document embeddings cho tất cả văn bản\n",
    "    all_embeddings = get_document_embeddings(model, all_texts, text_dataset)\n",
    "    \n",
    "    # Tính độ tương tự\n",
    "    similarities = compute_similarity(query_embedding, all_embeddings)\n",
    "    \n",
    "    # Lấy top_n văn bản tương tự nhất\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n+1]  # +1 vì có thể bao gồm chính query\n",
    "    \n",
    "    # Trả về kết quả\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        if all_texts[i] != query_text:  # Bỏ qua nếu là chính query\n",
    "            results.append({\n",
    "                'text': all_texts[i],\n",
    "                'similarity': similarities[i]\n",
    "            })\n",
    "    \n",
    "    return results[:top_n]\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "query = \"Deep learning is changing how we build AI systems.\"\n",
    "similar_docs = find_similar_documents(query, best_model, text_dataset, all_texts, top_n=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nCác văn bản tương tự:\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"\\n{i+1}. {doc['text']}\")\n",
    "    print(f\"   Similarity: {doc['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 9. Kết luận\n",
    "\n",
    "Chúng ta đã thực hiện việc biểu diễn Doc2vec sử dụng Deep Learning với pretrained models:\n",
    "\n",
    "1. **Pretrained Word Embeddings**: Sử dụng GloVe để khởi tạo word embeddings\n",
    "2. **Kiến trúc Deep Learning**: Xây dựng mô hình với nhiều lớp ẩn (3 lớp), batch normalization và dropout\n",
    "3. **Document Embeddings**: Tạo các vector biểu diễn cho văn bản bằng cách kết hợp word embeddings và doc embeddings\n",
    "\n",
    "Ưu điểm của phương pháp này:\n",
    "- Tận dụng được kiến thức từ pretrained word embeddings (transfer learning)\n",
    "- Mô hình deep learning cho phép học các biểu diễn phức tạp hơn\n",
    "- Document embeddings có thể sử dụng cho nhiều tác vụ NLP khác nhau\n",
    "\n",
    "Đây là một cách tiếp cận hiệu quả để biểu diễn văn bản dưới dạng vector, kết hợp được cả pretrained knowledge và khả năng học của deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
