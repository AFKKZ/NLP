{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\NguyenKhang\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\NguyenKhang\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thinc\n",
      "  Downloading thinc-9.1.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting tsfresh\n",
      "  Downloading tsfresh-0.21.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc)\n",
      "  Downloading blis-1.0.2-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting murmurhash<1.1.0,>=1.0.2 (from thinc)\n",
      "  Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from thinc)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from thinc)\n",
      "  Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting wasabi<1.2.0,>=0.8.1 (from thinc)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.0 (from thinc)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.4 (from thinc)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from thinc) (65.5.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.0.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (2.1.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from thinc)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (24.2)\n",
      "Requirement already satisfied: requests>=2.9.1 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tsfresh) (2.32.3)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tsfresh) (2.2.3)\n",
      "Collecting statsmodels>=0.13 (from tsfresh)\n",
      "  Downloading statsmodels-0.14.4-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting patsy>=0.4.1 (from tsfresh)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pywavelets (from tsfresh)\n",
      "  Downloading pywavelets-1.8.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tsfresh) (1.6.1)\n",
      "Requirement already satisfied: tqdm>=4.10.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tsfresh) (4.67.1)\n",
      "Collecting stumpy>=1.7.2 (from tsfresh)\n",
      "  Downloading stumpy-1.13.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting cloudpickle (from tsfresh)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: scipy>=1.14.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tsfresh) (1.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.25.0->tsfresh) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.25.0->tsfresh) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.25.0->tsfresh) (2025.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.9.1->tsfresh) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.9.1->tsfresh) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.9.1->tsfresh) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.9.1->tsfresh) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22.0->tsfresh) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22.0->tsfresh) (3.6.0)\n",
      "Collecting numba>=0.57.1 (from stumpy>=1.7.2->tsfresh)\n",
      "  Downloading numba-0.61.2-cp311-cp311-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.10.0->tsfresh) (0.4.6)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.57.1->stumpy>=1.7.2->tsfresh)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->tsfresh) (1.17.0)\n",
      "Downloading thinc-9.1.1-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.0/1.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading tsfresh-0.21.0-py2.py3-none-any.whl (96 kB)\n",
      "Downloading blis-1.0.2-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.6/2.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading statsmodels-0.14.4-cp311-cp311-win_amd64.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.9 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/9.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading stumpy-1.13.0-py3-none-any.whl (176 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading pywavelets-1.8.0-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.0/4.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.2 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading numba-0.61.2-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/30.3 MB 7.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.9/30.3 MB 7.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.7/30.3 MB 7.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 6.3/30.3 MB 7.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 7.9/30.3 MB 7.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 9.7/30.3 MB 7.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 11.5/30.3 MB 8.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 13.1/30.3 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 14.7/30.3 MB 7.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 16.0/30.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 17.3/30.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 19.1/30.3 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 20.7/30.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 22.5/30.3 MB 7.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 24.4/30.3 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 26.2/30.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 28.6/30.3 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 8.1 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, pywavelets, pydantic-core, patsy, murmurhash, llvmlite, cloudpickle, catalogue, blis, annotated-types, srsly, pydantic, preshed, numba, stumpy, statsmodels, confection, tsfresh, thinc\n",
      "Successfully installed annotated-types-0.7.0 blis-1.0.2 catalogue-2.0.10 cloudpickle-3.1.1 confection-0.1.5 cymem-2.0.11 llvmlite-0.44.0 murmurhash-1.0.13 numba-0.61.2 patsy-1.0.1 preshed-3.0.10 pydantic-2.11.5 pydantic-core-2.33.2 pywavelets-1.8.0 srsly-2.5.1 statsmodels-0.14.4 stumpy-1.13.0 thinc-9.1.1 tsfresh-0.21.0 typing-inspection-0.4.1 wasabi-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\NguyenKhang\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nguyenkhang\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.6/24.0 MB 4.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.7/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.7/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.2/24.0 MB 4.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 4.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 7.9/24.0 MB 4.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.2/24.0 MB 5.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.2/24.0 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.0 MB 5.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.4/24.0 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.5/24.0 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.0 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.6/24.0 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.9/24.0 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.2/24.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/15.8 MB 7.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/15.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.8 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.8/15.8 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.8 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.5/15.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/46.2 MB 8.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 3.1/46.2 MB 7.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 4.7/46.2 MB 8.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 7.9 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 7.1/46.2 MB 7.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 8.1/46.2 MB 6.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 9.4/46.2 MB 6.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 11.0/46.2 MB 6.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 12.6/46.2 MB 6.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 14.4/46.2 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 16.5/46.2 MB 7.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 17.8/46.2 MB 7.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 18.9/46.2 MB 7.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 7.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 22.0/46.2 MB 7.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 23.3/46.2 MB 7.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 25.2/46.2 MB 7.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.7/46.2 MB 7.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 28.3/46.2 MB 7.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 30.1/46.2 MB 7.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 32.0/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 33.3/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.9/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.4/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 38.0/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.9/46.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.3/46.2 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.8/46.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/46.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\NguyenKhang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\NguyenKhang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.2 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 9.1.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\NguyenKhang\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Các thư viện phụ thuộc cần có\n",
    "%pip install numpy>=2.0.0\n",
    "%pip install scipy>=1.14.0\n",
    "%pip install thinc tsfresh\n",
    "%pip install gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f92807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\NguyenKhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Thiết lập logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Tải NLTK data\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223b01e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo file sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Tạo dữ liệu mẫu\n",
    "sample_texts = [\n",
    "    \"This is a sample document about machine learning.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"Python is a popular programming language for AI.\",\n",
    "    \"Data science combines statistics and programming.\",\n",
    "    \"Neural networks are inspired by biological brains.\",\n",
    "    \"Machine learning algorithms learn from data.\",\n",
    "    \"Deep learning models require large datasets.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Computer vision helps machines understand images.\"\n",
    "]\n",
    "\n",
    "# Tạo DataFrame và lưu thành CSV\n",
    "df = pd.DataFrame({'text': sample_texts})\n",
    "df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Đã tạo file sample_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "import os\n",
    "\n",
    "def download_word2vec():\n",
    "    pretrained_path = 'word2vec_pretrained.bin'\n",
    "    \n",
    "    # Kiểm tra xem file đã tồn tại chưa\n",
    "    if not os.path.exists(pretrained_path):\n",
    "        print(\"Đang tải pretrained Word2Vec model...\")\n",
    "        word2vec_model = load('word2vec-google-news-300')\n",
    "        print(\"Đang lưu model...\")\n",
    "        word2vec_model.save_word2vec_format(pretrained_path, binary=True)\n",
    "        print(f\"Đã lưu model tại: {pretrained_path}\")\n",
    "    else:\n",
    "        print(f\"Pretrained model đã tồn tại tại: {pretrained_path}\")\n",
    "    \n",
    "    return pretrained_path\n",
    "\n",
    "# Tải và lưu model\n",
    "PRETRAINED_PATH = download_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71590b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, texts, max_length=1000):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset từ danh sách các văn bản\n",
    "        texts: list các văn bản\n",
    "        max_length: độ dài tối đa của mỗi văn bản\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.doc_vectors = []\n",
    "        \n",
    "        # Xây dựng từ điển\n",
    "        self._build_vocab()\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Xây dựng từ điển từ tất cả các văn bản\"\"\"\n",
    "        word_freq = defaultdict(int)\n",
    "        \n",
    "        # Đếm tần suất từ\n",
    "        for text in self.texts:\n",
    "            words = word_tokenize(text.lower())\n",
    "            for word in words:\n",
    "                word_freq[word] += 1\n",
    "        \n",
    "        # Tạo từ điển với các từ xuất hiện ít nhất 2 lần\n",
    "        words = [word for word, freq in word_freq.items() if freq >= 2]\n",
    "        for idx, word in enumerate(words):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Chuyển văn bản thành chuỗi các index\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        return [self.word2idx[word] for word in words \n",
    "                if word in self.word2idx][:self.max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af4c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecDataset(Dataset):\n",
    "    def __init__(self, text_dataset, window_size=5):\n",
    "        self.text_dataset = text_dataset\n",
    "        self.window_size = window_size\n",
    "        self.data = self._prepare_training_data()\n",
    "        \n",
    "    def _prepare_training_data(self):\n",
    "        \"\"\"Chuẩn bị dữ liệu huấn luyện với cửa sổ ngữ cảnh\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for doc_id, text in enumerate(self.text_dataset.texts):\n",
    "            word_indices = self.text_dataset.tokenize_text(text)\n",
    "            \n",
    "            for target_pos, target_idx in enumerate(word_indices):\n",
    "                # Lấy các từ trong cửa sổ ngữ cảnh\n",
    "                context_indices = []\n",
    "                for i in range(-self.window_size, self.window_size + 1):\n",
    "                    context_pos = target_pos + i\n",
    "                    if i != 0 and 0 <= context_pos < len(word_indices):\n",
    "                        context_indices.append(word_indices[context_pos])\n",
    "                \n",
    "                for context_idx in context_indices:\n",
    "                    training_data.append((target_idx, doc_id, context_idx))\n",
    "                    \n",
    "        return training_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target_idx, doc_id, context_idx = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(target_idx, dtype=torch.long),\n",
    "            torch.tensor(doc_id, dtype=torch.long),\n",
    "            torch.tensor(context_idx, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9b13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecWithPretrained(nn.Module):\n",
    "    def __init__(self, pretrained_path, vocab_size, doc_count, vector_size=300):\n",
    "        super(Doc2VecWithPretrained, self).__init__()\n",
    "        \n",
    "        # Tải pretrained word embeddings\n",
    "        self.word_vectors = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "        weights = torch.FloatTensor(self.word_vectors.vectors)\n",
    "        \n",
    "        # Khởi tạo embedding layers\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        self.doc_embeddings = nn.Embedding(doc_count, vector_size)\n",
    "        \n",
    "        # Khởi tạo doc embeddings\n",
    "        nn.init.uniform_(self.doc_embeddings.weight, -0.1, 0.1)\n",
    "        \n",
    "        # Layers\n",
    "        self.combine_layer = nn.Linear(vector_size * 2, vector_size)\n",
    "        self.output_layer = nn.Linear(vector_size, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, word_ids, doc_ids):\n",
    "        word_embeds = self.word_embeddings(word_ids)\n",
    "        doc_embeds = self.doc_embeddings(doc_ids)\n",
    "        \n",
    "        # Kết hợp embeddings\n",
    "        combined = torch.cat((word_embeds, doc_embeds), dim=1)\n",
    "        hidden = self.activation(self.combine_layer(combined))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.output_layer(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(model, train_loader, val_loader, epochs=10, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer với learning rates khác nhau\n",
    "    word_embed_params = {'params': model.word_embeddings.parameters(), 'lr': learning_rate * 0.1}\n",
    "    other_params = {'params': [p for n, p in model.named_parameters() \n",
    "                             if 'word_embeddings' not in n], 'lr': learning_rate}\n",
    "    \n",
    "    optimizer = optim.Adam([word_embed_params, other_params])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (words, docs, targets) in enumerate(train_loader):\n",
    "            words, docs, targets = words.to(device), docs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(words, docs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                logging.info(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for words, docs, targets in val_loader:\n",
    "                words, docs, targets = words.to(device), docs.to(device), targets.to(device)\n",
    "                outputs = model(words, docs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        logging.info(f'Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_doc2vec_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b2795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"Load and preprocess text data\"\"\"\n",
    "    # Đọc dữ liệu (giả sử là file CSV với cột 'text')\n",
    "    df = pd.read_csv(data_path)\n",
    "    texts = df['text'].tolist()\n",
    "    \n",
    "    # Chia train/val\n",
    "    train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_texts, val_texts\n",
    "\n",
    "def get_document_vector(model, text, text_dataset):\n",
    "    \"\"\"Lấy vector biểu diễn cho văn bản mới\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize văn bản\n",
    "        word_indices = text_dataset.tokenize_text(text)\n",
    "        if not word_indices:\n",
    "            return None\n",
    "        \n",
    "        # Tính trung bình các word vectors\n",
    "        word_vectors = model.word_embeddings(torch.tensor(word_indices))\n",
    "        doc_vector = word_vectors.mean(dim=0)\n",
    "        \n",
    "        return doc_vector.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddb13ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Không tìm thấy pretrained model: word2vec_pretrained.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHuấn luyện hoàn tất!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKhông tìm thấy file dữ liệu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(PRETRAINED_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKhông tìm thấy pretrained model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRETRAINED_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mĐang load dữ liệu...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load dữ liệu\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Không tìm thấy pretrained model: word2vec_pretrained.bin"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Tham số\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    VECTOR_SIZE = 300\n",
    "    WINDOW_SIZE = 5\n",
    "    \n",
    "    # Đường dẫn files\n",
    "    DATA_PATH = \"sample_data.csv\"\n",
    "    PRETRAINED_PATH = \"word2vec_pretrained.bin\"  # phải trùng với tên file đã tải ở cell trước\n",
    "    \n",
    "    # Kiểm tra files tồn tại\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"Không tìm thấy file dữ liệu: {DATA_PATH}\")\n",
    "    if not os.path.exists(PRETRAINED_PATH):\n",
    "        raise FileNotFoundError(f\"Không tìm thấy pretrained model: {PRETRAINED_PATH}\")\n",
    "    \n",
    "    print(\"Đang load dữ liệu...\")\n",
    "    # Load dữ liệu\n",
    "    train_texts, val_texts = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    print(\"Khởi tạo datasets...\")\n",
    "    # Khởi tạo datasets\n",
    "    train_text_dataset = TextDataset(train_texts)\n",
    "    val_text_dataset = TextDataset(val_texts)\n",
    "    \n",
    "    train_dataset = Doc2VecDataset(train_text_dataset, window_size=WINDOW_SIZE)\n",
    "    val_dataset = Doc2VecDataset(val_text_dataset, window_size=WINDOW_SIZE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(\"Khởi tạo model...\")\n",
    "    # Khởi tạo model\n",
    "    model = Doc2VecWithPretrained(\n",
    "        pretrained_path=PRETRAINED_PATH,\n",
    "        vocab_size=train_text_dataset.vocab_size,\n",
    "        doc_count=len(train_texts),\n",
    "        vector_size=VECTOR_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"Bắt đầu huấn luyện...\")\n",
    "    # Train model\n",
    "    train_doc2vec(model, train_loader, val_loader, EPOCHS, LEARNING_RATE)\n",
    "    print(\"Huấn luyện hoàn tất!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
