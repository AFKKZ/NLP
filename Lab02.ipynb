{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n6JKetXIPjbj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"apple\", \"banana\", \"grapes\", \"watermelon\", \"orange\",\n",
        "    \"the cat\", \"the dog\", \"a cat\", \"a dog\", \"my dog\",\n",
        "    \"the cat sleeps\", \"the dog barks\", \"a cat eats\", \"a dog runs\",\n",
        "    \"my dog sleeps\", \"my cat meows\", \"a bird flies\", \"the bird sings\",\n",
        "    \"the sun shines\", \"the wind blows\"\n",
        "]"
      ],
      "metadata": {
        "id": "iACra1OmPqSX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [[\"<bos>\"] + s.split() + [\"<eos>\"] for s in corpus]"
      ],
      "metadata": {
        "id": "lUUFY8H3WxpQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect n-gram counts\n",
        "def get_ngram_counts(sentences, n):\n",
        "    counts = defaultdict(Counter)\n",
        "    for sent in sentences:\n",
        "        for i in range(n - 1, len(sent)):\n",
        "            context = tuple(sent[i - n + 1:i])\n",
        "            word = sent[i]\n",
        "            counts[context][word] += 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "# Calculate experimental probability p*\n",
        "def get_prob(counts):\n",
        "    prob = {}\n",
        "    for context in counts:\n",
        "        total = sum(counts[context].values())\n",
        "        prob[context] = {w: c / total for w, c in counts[context].items()}\n",
        "    return prob"
      ],
      "metadata": {
        "id": "RhLMgyD-em8e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm to learn interpolation weights\n",
        "def em_interpolation(sentences, probs, n_max, iterations=10):\n",
        "    # Number of actual word\n",
        "    M = sum(\n",
        "        1 for sent in sentences for tok in sent\n",
        "        if tok not in (\"<bos>\", \"<eos>\")\n",
        "    )\n",
        "    print(f\"Number of words: {M}\")\n",
        "\n",
        "    # Initialization lambda with uniform distribution\n",
        "    lambdas = np.array([1 / n_max] * n_max)\n",
        "\n",
        "    for it in range(iterations):\n",
        "        # responsibility matrix for posterior probability\n",
        "        q = np.zeros((M, n_max))\n",
        "        token_index = 0\n",
        "\n",
        "        # E-step: estimate posterior probability q\n",
        "        for sent in sentences:\n",
        "            for i in range(0, len(sent)):\n",
        "                word = sent[i]\n",
        "                if word in (\"<bos>\", \"<eos>\"):\n",
        "                    continue\n",
        "                context_probs = []\n",
        "                for z in range(1, n_max + 1):\n",
        "                    context = tuple(sent[max(i - z, 0):i])\n",
        "                    prob = probs[z - 1].get(context, {}).get(word, 1e-10)   # avoid zero prob\n",
        "                    context_probs.append(prob * lambdas[z - 1])\n",
        "\n",
        "                # Normalize(q)\n",
        "                norm = sum(context_probs)\n",
        "                if norm == 0:\n",
        "                    norm = 1e-10\n",
        "                q[token_index] = [cp / norm for cp in context_probs]\n",
        "                token_index += 1\n",
        "\n",
        "        # M-step: update lambda\n",
        "        lambdas = np.mean(q, axis=0)\n",
        "        lambdas /= np.sum(lambdas)\n",
        "        print(f\"Iteration {it + 1}: lambdas = {lambdas}\")\n",
        "\n",
        "    return lambdas"
      ],
      "metadata": {
        "id": "t28pFc7VXT24"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute counts and probabilities for 1-gram, 2-gram, and 3-gram\n",
        "counts = [get_ngram_counts(sentences, n) for n in [1, 2, 3]]\n",
        "probs = [get_prob(c) for c in counts]\n",
        "n_max = 3\n",
        "\n",
        "# Run EM\n",
        "lambdas = em_interpolation(sentences, probs, n_max)\n",
        "print(f\"\\nFinal interpolated weights: {lambdas}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgHF32T9Qkqz",
        "outputId": "36b68475-6cc8-4a72-8aa4-6c0d4d95b71e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 45\n",
            "Iteration 1: lambdas = [0.07407407 0.51851852 0.40740741]\n",
            "Iteration 2: lambdas = [0.01646091 0.55967078 0.42386831]\n",
            "Iteration 3: lambdas = [0.00365798 0.56881573 0.42752629]\n",
            "Iteration 4: lambdas = [0.00081288 0.57084794 0.42833918]\n",
            "Iteration 5: lambdas = [1.80640941e-04 5.71299542e-01 4.28519817e-01]\n",
            "Iteration 6: lambdas = [4.01424315e-05 5.71399898e-01 4.28559959e-01]\n",
            "Iteration 7: lambdas = [8.92054036e-06 5.71422199e-01 4.28568880e-01]\n",
            "Iteration 8: lambdas = [1.98234231e-06 5.71427155e-01 4.28570862e-01]\n",
            "Iteration 9: lambdas = [4.40520515e-07 5.71428257e-01 4.28571303e-01]\n",
            "Iteration 10: lambdas = [9.78934482e-08 5.71428501e-01 4.28571401e-01]\n",
            "\n",
            "Final interpolated weights: [9.78934482e-08 5.71428501e-01 4.28571401e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute interpolated probability of a word given context\n",
        "def interpolated_prob(word, context_tokens, probs, lambdas):\n",
        "    total_prob = 0.0\n",
        "    return total_prob\n",
        "\n",
        "# Examples\n",
        "examples = [\n",
        "    {'context': [\"my\", \"cat\"], 'word': \"sleeps\"},\n",
        "    {'context': [\"my\", \"dog\"], 'word': \"sleeps\"}\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    prob_on = interpolated_prob(ex['word'], ex['context'], probs, lambdas)\n",
        "    print(f\"Interpolated probability: P('{ex['word']}'|'{' '.join(ex['context'])}') = {prob_on:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAPkWsvU0vB",
        "outputId": "d4156293-9d55-4bb1-d5f8-3f21151109ac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolated probability: P('sleeps'|'my cat') = 0.0000\n",
            "Interpolated probability: P('sleeps'|'my dog') = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JupUc-6U4t9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}