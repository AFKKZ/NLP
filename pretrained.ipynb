{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các thư viện phụ thuộc cần có\n",
    "%pip install numpy>=2.0.0\n",
    "%pip install scipy>=1.14.0\n",
    "%pip install thinc tsfresh\n",
    "%pip install gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Thiết lập logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Tải NLTK data\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo dữ liệu mẫu\n",
    "sample_texts = [\n",
    "    \"This is a sample document about machine learning.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"Python is a popular programming language for AI.\",\n",
    "    \"Data science combines statistics and programming.\",\n",
    "    \"Neural networks are inspired by biological brains.\",\n",
    "    \"Machine learning algorithms learn from data.\",\n",
    "    \"Deep learning models require large datasets.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Computer vision helps machines understand images.\"\n",
    "]\n",
    "\n",
    "# Tạo DataFrame và lưu thành CSV\n",
    "df = pd.DataFrame({'text': sample_texts})\n",
    "df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Đã tạo file sample_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "import os\n",
    "\n",
    "def download_word2vec():\n",
    "    pretrained_path = 'word2vec_pretrained.bin'\n",
    "    \n",
    "    # Kiểm tra xem file đã tồn tại chưa\n",
    "    if not os.path.exists(pretrained_path):\n",
    "        print(\"Đang tải pretrained Word2Vec model...\")\n",
    "        word2vec_model = load('word2vec-google-news-300')\n",
    "        print(\"Đang lưu model...\")\n",
    "        word2vec_model.save_word2vec_format(pretrained_path, binary=True)\n",
    "        print(f\"Đã lưu model tại: {pretrained_path}\")\n",
    "    else:\n",
    "        print(f\"Pretrained model đã tồn tại tại: {pretrained_path}\")\n",
    "    \n",
    "    return pretrained_path\n",
    "\n",
    "# Tải và lưu model\n",
    "PRETRAINED_PATH = download_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71590b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, texts, max_length=1000):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset từ danh sách các văn bản\n",
    "        texts: list các văn bản\n",
    "        max_length: độ dài tối đa của mỗi văn bản\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.doc_vectors = []\n",
    "        \n",
    "        # Xây dựng từ điển\n",
    "        self._build_vocab()\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Xây dựng từ điển từ tất cả các văn bản\"\"\"\n",
    "        word_freq = defaultdict(int)\n",
    "        \n",
    "        # Đếm tần suất từ\n",
    "        for text in self.texts:\n",
    "            words = word_tokenize(text.lower())\n",
    "            for word in words:\n",
    "                word_freq[word] += 1\n",
    "        \n",
    "        # Tạo từ điển với các từ xuất hiện ít nhất 2 lần\n",
    "        words = [word for word, freq in word_freq.items() if freq >= 2]\n",
    "        for idx, word in enumerate(words):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Chuyển văn bản thành chuỗi các index\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        return [self.word2idx[word] for word in words \n",
    "                if word in self.word2idx][:self.max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af4c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecDataset(Dataset):\n",
    "    def __init__(self, text_dataset, window_size=5):\n",
    "        self.text_dataset = text_dataset\n",
    "        self.window_size = window_size\n",
    "        self.data = self._prepare_training_data()\n",
    "        \n",
    "    def _prepare_training_data(self):\n",
    "        \"\"\"Chuẩn bị dữ liệu huấn luyện với cửa sổ ngữ cảnh\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for doc_id, text in enumerate(self.text_dataset.texts):\n",
    "            word_indices = self.text_dataset.tokenize_text(text)\n",
    "            \n",
    "            for target_pos, target_idx in enumerate(word_indices):\n",
    "                # Lấy các từ trong cửa sổ ngữ cảnh\n",
    "                context_indices = []\n",
    "                for i in range(-self.window_size, self.window_size + 1):\n",
    "                    context_pos = target_pos + i\n",
    "                    if i != 0 and 0 <= context_pos < len(word_indices):\n",
    "                        context_indices.append(word_indices[context_pos])\n",
    "                \n",
    "                for context_idx in context_indices:\n",
    "                    training_data.append((target_idx, doc_id, context_idx))\n",
    "                    \n",
    "        return training_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target_idx, doc_id, context_idx = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(target_idx, dtype=torch.long),\n",
    "            torch.tensor(doc_id, dtype=torch.long),\n",
    "            torch.tensor(context_idx, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9b13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecWithPretrained(nn.Module):\n",
    "    def __init__(self, pretrained_path, vocab_size, doc_count, vector_size=300):\n",
    "        super(Doc2VecWithPretrained, self).__init__()\n",
    "        \n",
    "        # Tải pretrained word embeddings\n",
    "        self.word_vectors = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "        weights = torch.FloatTensor(self.word_vectors.vectors)\n",
    "        \n",
    "        # Khởi tạo embedding layers\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        self.doc_embeddings = nn.Embedding(doc_count, vector_size)\n",
    "        \n",
    "        # Khởi tạo doc embeddings\n",
    "        nn.init.uniform_(self.doc_embeddings.weight, -0.1, 0.1)\n",
    "        \n",
    "        # Layers\n",
    "        self.combine_layer = nn.Linear(vector_size * 2, vector_size)\n",
    "        self.output_layer = nn.Linear(vector_size, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, word_ids, doc_ids):\n",
    "        word_embeds = self.word_embeddings(word_ids)\n",
    "        doc_embeds = self.doc_embeddings(doc_ids)\n",
    "        \n",
    "        # Kết hợp embeddings\n",
    "        combined = torch.cat((word_embeds, doc_embeds), dim=1)\n",
    "        hidden = self.activation(self.combine_layer(combined))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.output_layer(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(model, train_loader, val_loader, epochs=10, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer với learning rates khác nhau\n",
    "    word_embed_params = {'params': model.word_embeddings.parameters(), 'lr': learning_rate * 0.1}\n",
    "    other_params = {'params': [p for n, p in model.named_parameters() \n",
    "                             if 'word_embeddings' not in n], 'lr': learning_rate}\n",
    "    \n",
    "    optimizer = optim.Adam([word_embed_params, other_params])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (words, docs, targets) in enumerate(train_loader):\n",
    "            words, docs, targets = words.to(device), docs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(words, docs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                logging.info(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for words, docs, targets in val_loader:\n",
    "                words, docs, targets = words.to(device), docs.to(device), targets.to(device)\n",
    "                outputs = model(words, docs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        logging.info(f'Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_doc2vec_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b2795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"Load and preprocess text data\"\"\"\n",
    "    # Đọc dữ liệu (giả sử là file CSV với cột 'text')\n",
    "    df = pd.read_csv(data_path)\n",
    "    texts = df['text'].tolist()\n",
    "    \n",
    "    # Chia train/val\n",
    "    train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_texts, val_texts\n",
    "\n",
    "def get_document_vector(model, text, text_dataset):\n",
    "    \"\"\"Lấy vector biểu diễn cho văn bản mới\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize văn bản\n",
    "        word_indices = text_dataset.tokenize_text(text)\n",
    "        if not word_indices:\n",
    "            return None\n",
    "        \n",
    "        # Tính trung bình các word vectors\n",
    "        word_vectors = model.word_embeddings(torch.tensor(word_indices))\n",
    "        doc_vector = word_vectors.mean(dim=0)\n",
    "        \n",
    "        return doc_vector.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Tham số\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    VECTOR_SIZE = 300\n",
    "    WINDOW_SIZE = 5\n",
    "    \n",
    "    # Đường dẫn files\n",
    "    DATA_PATH = \"sample_data.csv\"\n",
    "    PRETRAINED_PATH = \"word2vec_pretrained.bin\"  # phải trùng với tên file đã tải ở cell trước\n",
    "    \n",
    "    # Kiểm tra files tồn tại\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"Không tìm thấy file dữ liệu: {DATA_PATH}\")\n",
    "    if not os.path.exists(PRETRAINED_PATH):\n",
    "        raise FileNotFoundError(f\"Không tìm thấy pretrained model: {PRETRAINED_PATH}\")\n",
    "    \n",
    "    print(\"Đang load dữ liệu...\")\n",
    "    # Load dữ liệu\n",
    "    train_texts, val_texts = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    print(\"Khởi tạo datasets...\")\n",
    "    # Khởi tạo datasets\n",
    "    train_text_dataset = TextDataset(train_texts)\n",
    "    val_text_dataset = TextDataset(val_texts)\n",
    "    \n",
    "    train_dataset = Doc2VecDataset(train_text_dataset, window_size=WINDOW_SIZE)\n",
    "    val_dataset = Doc2VecDataset(val_text_dataset, window_size=WINDOW_SIZE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(\"Khởi tạo model...\")\n",
    "    # Khởi tạo model\n",
    "    model = Doc2VecWithPretrained(\n",
    "        pretrained_path=PRETRAINED_PATH,\n",
    "        vocab_size=train_text_dataset.vocab_size,\n",
    "        doc_count=len(train_texts),\n",
    "        vector_size=VECTOR_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"Bắt đầu huấn luyện...\")\n",
    "    # Train model\n",
    "    train_doc2vec(model, train_loader, val_loader, EPOCHS, LEARNING_RATE)\n",
    "    print(\"Huấn luyện hoàn tất!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
